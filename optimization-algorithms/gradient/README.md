# الگوریتم‌های بهینه‌سازی گرادیانی در پایتون

این پروژه پیاده‌سازی سه الگوریتم پرکاربرد برای کمینه‌سازی یک تابع هدف را نشان می‌دهد:

- گرادیان نزولی (Gradient Descent)
- مومنتوم (Momentum)
- آدام (Adam)

---

## 🧠 تابع هدف

تابعی که قصد کمینه‌سازی آن را داریم به صورت زیر است:

f(x, y) = sin(x) * cos(y) + 0.1 * (x^2 + y^2)

این تابع شامل ترکیبی از توابع مثلثاتی و درجه دوم است که منظره‌ای غیر محدب (non-convex) ایجاد می‌کند و برای تست الگوریتم‌های بهینه‌سازی مناسب است.

---

## 📐 گرادیان‌ها (مشتق‌های جزئی)

گرادیان تابع به صورت زیر محاسبه می‌شود:

- df/dx = cos(x) * cos(y) + 0.2 * x
- df/dy = -sin(x) * sin(y) + 0.2 * y

تمام الگوریتم‌ها از این گرادیان برای به‌روزرسانی موقعیت استفاده می‌کنند.

---

## 🚀 الگوریتم‌های بهینه‌سازی (نحوه عملکرد)

### 1. گرادیان نزولی (Gradient Descent)

در این روش، در جهت مخالف گرادیان حرکت می‌کنیم تا به کمینه محلی برسیم.

**مراحل اجرا:**

1. انتخاب نقطه شروع تصادفی.
2. محاسبه گرادیان در آن نقطه.
3. به‌روزرسانی پارامترها با کم کردن گرادیان ضربدر نرخ یادگیری.
4. تکرار مراحل بالا برای تعداد مشخصی گام.

**فرمول:**

- x_new = x_old - eta * grad_f(x)


---

### 2. مومنتوم (Momentum)

با اضافه کردن یک بردار سرعت به الگوریتم، همگرایی را سریع‌تر می‌کند و از نوسانات شدید جلوگیری می‌کند.

**مراحل اجرا:**

1. مقداردهی اولیه سرعت برابر صفر.
2. در هر مرحله:
   - محاسبه سرعت جدید با استفاده از سرعت قبلی و گرادیان فعلی.
   - به‌روزرسانی موقعیت با استفاده از سرعت.

**فرمول‌ها:**

- v_t = beta * v_t{t-1} + (1 - beta) * grad_f(x)
- x = x - eta * v_t


---

### 3. آدام (Adam)

ترکیبی از مومنتوم و RMSProp است که نرخ یادگیری را برای هر پارامتر به‌صورت تطبیقی تنظیم می‌کند.

**مراحل اجرا:**

1. مقداردهی اولیه m و v برابر صفر.
2. به‌روزرسانی میانگین گرادیان و میانگین مربع گرادیان.
3. تصحیح اریبی (bias correction).
4. به‌روزرسانی پارامترها با استفاده از میانگین‌ها.

**فرمول‌ها:**

- m_t = beta1 * m_{t-1} + (1 - beta1) * grad_f(x)
- v_t = beta2 * v_{t-1} + (1 - beta2) * (grad_f(x))^2
- m_hat = m_t / (1 - beta1^t)
- v_hat = v_t / (1 - beta2^t)
- x = x - eta * m_hat / (sqrt(v_hat) + epsilon)





